{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kvih9mXkNN"
      },
      "source": [
        "# **Videos Transcription and Translation with Faster Whisper and ChatGPT**\n",
        "\n",
        "\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/lewangdev/autotranslate/blob/main/autotranslate.ipynb)](https://colab.research.google.com/github/lewangdev/autotranslate/blob/main/autotranslate.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/lewangdev/autotranslate)](https://github.com/lewangdev/autotranslate)\n",
        "\n",
        "This Notebook will guide you through the transcription and translation of video using [Faster Whisper](https://github.com/guillaumekln/faster-whisper) and ChatGPT. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript, translation and video audio in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QshUbLqpX7L4"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Check GPU type** ðŸ•µï¸\n",
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Install libraries** ðŸ—ï¸\n",
        "#@markdown This cell will take a little while to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "! pip install faster-whisper==0.10.0\n",
        "! pip install yt-dlp==2023.11.16\n",
        "! pip install openai==0.28.1\n",
        "\n",
        "# Windows Libsï¼šhttps://github.com/Purfview/whisper-standalone-win/releases/download/libs/cuBLAS.and.cuDNN_win_v2.7z\n",
        "! apt install -y p7zip-full p7zip-rar\n",
        "! wget https://github.com/Purfview/whisper-standalone-win/releases/download/libs/cuBLAS.and.cuDNN_linux_v2.7z\n",
        "! 7z x cuBLAS.and.cuDNN_linux_v2.7z -o/usr/lib\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DDX38HH5xLot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IfG0E_WbRFI0"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Import libraries for Python** ðŸ\n",
        "\n",
        "#@markdown This cell will import all libraries for python code.\n",
        "import sys\n",
        "import warnings\n",
        "from faster_whisper import WhisperModel\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zwGAsr4sIgd"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Optional:** Save data in Google Drive ðŸ’¾\n",
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Videos Transcription and Translation\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TMhrSq_GZ6kA"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Model selection** ðŸ§ \n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~0.8 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1.0 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~1.4 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~2.7 GB     |      ~2x       |\n",
        "#@markdown | large-v1  |   1550 M   |        N/A         |      `large-v1`       |    ~4.3 GB     |       1x       |\n",
        "#@markdown | large-v2  |   1550 M   |        N/A         |      `large-v2`       |    ~4.3 GB     |       1x       |\n",
        "#@markdown | large-v3  |   1550 M   |        N/A         |      `large-v2`       |    ~3.6 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "model_size = 'large-v2' #@param ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium', 'medium.en', 'large-v1', 'large-v2', 'large-v3']\n",
        "device_type = \"cuda\" #@param {type:\"string\"} ['cuda', 'cpu']\n",
        "compute_type = \"float16\" #@param {type:\"string\"} ['float16', 'int8_float16', 'int8']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "model = WhisperModel(model_size, device=device_type, compute_type=compute_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** ðŸ“º\n",
        "\n",
        "#@markdown Enter the URL of the video you want to transcribe\n",
        "\n",
        "#@markdown #### **Video or playlist URL**\n",
        "URL = \"https://youtu.be/pTCxXZh6VyE?si=ItBkmwhihuxInLjp\" #@param {type:\"string\"}\n",
        "# store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "def download_video(URL):\n",
        "  video_path_local_list = []\n",
        "\n",
        "  ydl_opts = {\n",
        "        'format': 'm4a/bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',\n",
        "        # â„¹ï¸ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "        'postprocessors': [{  # Extract audio using ffmpeg\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "        }]\n",
        "    }\n",
        "\n",
        "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        error_code = ydl.download([URL])\n",
        "        list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "  for video_info in list_video_info:\n",
        "        video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "  print(video_path_local_list)\n",
        "  for video_path_local in video_path_local_list:\n",
        "      if video_path_local.suffix == \".mp4\":\n",
        "          video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "          result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n",
        "  return video_path_local\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AHjPu7qlPPVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(video_path_local):\n",
        "  #@markdown # **Run the model** ðŸš€\n",
        "\n",
        "  #@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "  def seconds_to_time_format(s):\n",
        "      # Convert seconds to hours, minutes, seconds, and milliseconds\n",
        "      hours = s // 3600\n",
        "      s %= 3600\n",
        "      minutes = s // 60\n",
        "      s %= 60\n",
        "      seconds = s // 1\n",
        "      milliseconds = round((s % 1) * 1000)\n",
        "\n",
        "      # Return the formatted string\n",
        "      return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{int(milliseconds):03d}\"\n",
        "\n",
        "\n",
        "  #@markdown ## **Parameters** âš™ï¸\n",
        "\n",
        "  #@markdown ### **Behavior control**\n",
        "  #@markdown #### Language\n",
        "  language_options = {\n",
        "      \"Auto Detect\": \"auto\",\n",
        "      \"English\": \"en\",\n",
        "      \"ä¸­æ–‡(Chinese)\": \"zh\",\n",
        "      \"æ—¥æœ¬èªž(Japanese)\": \"ja\",\n",
        "      \"Deutsch(German)\": \"de\",\n",
        "      \"FranÃ§ais(French)\": \"fr\"\n",
        "  }\n",
        "\n",
        "  language_option = \"Auto Detect\" #@param [\"Auto Detect\", \"English\", \"ä¸­æ–‡(Chinese)\", \"æ—¥æœ¬èªž(Japanese)\", \"Deutsch(German)\", \"FranÃ§ais(French)\"] {allow-input: true}\n",
        "  language = language_options.get(language_option, language_option)\n",
        "\n",
        "  #@markdown #### initial prompt\n",
        "  initial_prompt = \"Hello, Let's begin to talk.\" #@param {type:\"string\"}\n",
        "  #@markdown ---\n",
        "  #@markdown #### Word-level timestamps\n",
        "  word_level_timestamps = True #@param {type:\"boolean\"}\n",
        "  #@markdown ---\n",
        "  #@markdown #### VAD filter\n",
        "  vad_filter = False #@param {type:\"boolean\"}\n",
        "  vad_filter_min_silence_duration_ms = 50 #@param {type:\"integer\"}\n",
        "  #@markdown ---\n",
        "\n",
        "\n",
        "  segments, info = model.transcribe(str(video_path_local), beam_size=5,\n",
        "                                    language=None if language == \"auto\" else language,\n",
        "                                    initial_prompt=initial_prompt,\n",
        "                                    word_timestamps=word_level_timestamps,\n",
        "                                    vad_filter=vad_filter,\n",
        "                                    vad_parameters=dict(min_silence_duration_ms=vad_filter_min_silence_duration_ms))\n",
        "\n",
        "  language_detected = info.language\n",
        "  display(Markdown(f\"Detected language '{info.language}' with probability {info.language_probability}\"))\n",
        "\n",
        "  fragments = []\n",
        "\n",
        "  for segment in segments:\n",
        "    print(f\"[{seconds_to_time_format(segment.start)} --> {seconds_to_time_format(segment.end)}] {segment.text}\")\n",
        "    if word_level_timestamps:\n",
        "      for word in segment.words:\n",
        "        ts_start = seconds_to_time_format(word.start)\n",
        "        ts_end = seconds_to_time_format(word.end)\n",
        "        #print(f\"[{ts_start} --> {ts_end}] {word.word}\")\n",
        "        fragments.append(dict(start=word.start,end=word.end,text=word.word))\n",
        "    else:\n",
        "      ts_start = seconds_to_time_format(segment.start)\n",
        "      ts_end = seconds_to_time_format(segment.end)\n",
        "      #print(f\"[{ts_start} --> {ts_end}] {segment.text}\")\n",
        "      fragments.append(dict(start=segment.start,end=segment.end,text=segment.text))\n",
        "\n",
        "  def merge_word_to_sentences():\n",
        "    #@title Merge words/segments to sentences\n",
        "\n",
        "    #@markdown Run this cell to merge words/segments to sentences.\n",
        "    #@markdown ## **Parameters** âš™ï¸\n",
        "\n",
        "    #@markdown ### **Behavior control**\n",
        "    #@markdown #### Milliseconds gap between_two sentences\n",
        "    max_gap_ms_between_two_sentence = 200 #@param {type:\"integer\"}\n",
        "\n",
        "    import json\n",
        "\n",
        "    # Merge words/segments to sentences\n",
        "    def merge_fragments(fragments, gap_ms):\n",
        "      new_fragments = []\n",
        "      new_fragment = {}\n",
        "      length = len(fragments)\n",
        "      for i, fragment in enumerate(fragments):\n",
        "        start = fragment['start']\n",
        "        end = fragment['end']\n",
        "        text = fragment['text']\n",
        "\n",
        "        if new_fragment.get('start', None) is None:\n",
        "          new_fragment['start'] = start\n",
        "        if new_fragment.get('end', None) is None:\n",
        "          new_fragment['end'] = end\n",
        "        if new_fragment.get('text', None) is None:\n",
        "          new_fragment['text'] = \"\"\n",
        "\n",
        "        if start - new_fragment['end'] > gap_ms:\n",
        "          new_fragments.append(new_fragment)\n",
        "          new_fragment = dict(start=start, end=end, text=text)\n",
        "          continue\n",
        "\n",
        "        new_fragment['end'] = end\n",
        "\n",
        "        #delimiter = '' if text.startswith('-') else ' '\n",
        "        delimiter = ' ' if language_detected in ['en', 'de', 'fr'] else ''\n",
        "        new_fragment['text'] = f\"{new_fragment['text']}{delimiter}{text.lstrip()}\"\n",
        "\n",
        "        # End of a sentence when symbols found: [.?]\n",
        "        if (len(text) > 0 and text[-1] in ['.', '?', 'ã€‚', 'ï¼Ÿ', '!', 'ï¼']) or i == length-1:\n",
        "          new_fragments.append(new_fragment)\n",
        "          new_fragment = {}\n",
        "      return new_fragments\n",
        "\n",
        "\n",
        "    new_fragments = merge_fragments(fragments, max_gap_ms_between_two_sentence/1000.0)\n",
        "\n",
        "    # Save as json file\n",
        "    json_ext_name = \".json\"\n",
        "    json_transcript_file_name = video_path_local.stem + json_ext_name\n",
        "    with open(json_transcript_file_name, 'w') as f:\n",
        "      f.write(json.dumps(new_fragments))\n",
        "    display(Markdown(f\"**Transcript SRT file created: {video_path_local.parent / json_transcript_file_name}**\"))\n",
        "\n",
        "    # Save as srt\n",
        "    srt_ext_name = \".srt\"\n",
        "    srt_transcript_file_name = video_path_local.stem + srt_ext_name\n",
        "    with open(srt_transcript_file_name, 'w') as f:\n",
        "      for sentence_idx, fragment in enumerate(new_fragments):\n",
        "        ts_start = seconds_to_time_format(fragment['start'])\n",
        "        ts_end = seconds_to_time_format(fragment['end'])\n",
        "        text = fragment['text']\n",
        "        print(f\"[{ts_start} --> {ts_end}] {text}\")\n",
        "        f.write(f\"{sentence_idx + 1}\\n\")\n",
        "        f.write(f\"{ts_start} --> {ts_end}\\n\")\n",
        "        f.write(f\"{text.strip()}\\n\\n\")\n",
        "\n",
        "    try:\n",
        "      shutil.copy(video_path_local.parent / srt_transcript_file_name,\n",
        "                drive_whisper_path / srt_transcript_file_name\n",
        "      )\n",
        "      display(Markdown(f\"**Transcript SRT file created: {drive_whisper_path / srt_transcript_file_name}**\"))\n",
        "      df.at[index, 'status'] = 'Transcribed'\n",
        "    except:\n",
        "      display(Markdown(f\"**Transcript SRT file created: {video_path_local.parent / srt_transcript_file_name}**\"))\n",
        "\n",
        "  merge_word_to_sentences()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W9eMqey8QDh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # **Mount Drive And Load CSV**\n",
        "# @markdown You can find path inside drive folder after mounting the drive.\n",
        "#@markdown CSV File path must be from Google Drive as we have to update it while transcribing.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "google_drive_csv_path = \"/content/drive/MyDrive/video_links.csv\" #@param {type:\"string\"}\n",
        "\n",
        "# @markdown If CSV file fails it will convert the Video Given above in Video Selection Section. So if you want to transcribe a single video you can just give  a dummy path in  this input.\n",
        "\n",
        "# Mount Google Drive to save and load checkpoints\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "#Read CSV File\n",
        "try:\n",
        "  df = pd.read_csv(google_drive_csv_path)\n",
        "  print(df)\n",
        "except:\n",
        "  video_path = download_video(URL)\n",
        "  run_model(video_path)"
      ],
      "metadata": {
        "id": "Xmw-3AmPxiNY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #Main Cell\n",
        "# @markdown This code block will run the loop through all the rows in CSV file and transcribe it. Once you run this code block it will transcribe all the videos and mark all the videos converetd s Transcribed so that they won't get transcribed again.\n",
        "for index, row in df.iterrows():\n",
        "    video_url = row['video_url']\n",
        "    status = row['status'].strip().lower()\n",
        "    print(video_url, \"-\", status)\n",
        "    # Check if the video is transcribed\n",
        "    if status != 'transcribed':\n",
        "      video_path = download_video(video_url)\n",
        "      run_model(video_path)\n",
        "      # Once the video is transcribed, update the status in the CSV file\n",
        "      df.to_csv('/content/drive/MyDrive/video_links.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GNb7oKYzXooH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}